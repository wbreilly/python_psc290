{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/WBR/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /Users/WBR/anaconda3/lib/python3.6/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import sys\n",
    "from os import path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from textblob import TextBlob\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab text messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab your iMessages courtesy of https://stmorse.github.io/journal/iMessage.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to message db\n",
    "conn = sqlite3.connect('/Users/wbr/Library/Messages/chat.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the messages from chat id 194, which is my the id for my Family group chat\n",
    "cmd1 = 'SELECT ROWID, text, handle_id, \\\n",
    "            datetime(date + strftime(\\'%s\\',\\'2001-01-01\\'), \\'unixepoch\\') as date_utc \\\n",
    "            FROM message T1 \\\n",
    "            INNER JOIN chat_message_join T2 \\\n",
    "                ON T2.chat_id=198 \\\n",
    "                AND T1.ROWID=T2.message_id \\\n",
    "            ORDER BY T1.date'\n",
    "c.execute(cmd1)\n",
    "df_msg = pd.DataFrame(c.fetchall(), columns=['id', 'text', 'sender', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49794"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a wordcloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wordcloud source https://github.com/nikhilkumarsingh/wordcloud-example/blob/master/mywc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get path to directory\n",
    "currdir = '/Users/WBR/walter/python_psc290/hw'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = np.array(Image.open(path.join(currdir, \"cloud.png\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_words = df_msg.text.str.cat(sep=',')\n",
    "cat_words = df_msg.text.to_string\n",
    "# print(cat_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ' '.join(df_msg['text'])\n",
    "# doesnt work because there are blank cells.\n",
    "# gonna fix this by replacing blanks with nan then dropping nan\n",
    "df2['text'].replace('', np.nan, inplace=True)\n",
    "df2.dropna(subset=['text'], inplace=True)\n",
    "# now conc everything the right way\n",
    "conc_messages = ' '.join(df2['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conc_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_wiki(query):\n",
    "#     # get best matching title for given query\n",
    "#     title = wikipedia.search(query)[0]\n",
    "\n",
    "#     # get wikipedia page for selected title\n",
    "#     page = wikipedia.page(title)\n",
    "#     return page.content\n",
    "\n",
    "\n",
    "def create_wordcloud(text):\n",
    "    # create numpy array for wordcloud mask image\n",
    "    mask = np.array(Image.open(path.join(currdir, \"cloud.png\")))\n",
    "\n",
    "    # create set of stopwords\t\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    # create wordcloud object\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "                    max_words=200, \n",
    "                    mask=mask,\n",
    "                    stopwords=stopwords)\n",
    "\n",
    "    # generate wordcloud\n",
    "#     wc.generate(text)\n",
    "    wc.generate_from_text(text)\n",
    "    # save wordcloud\n",
    "    wc.to_file(path.join(currdir, \"wc.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(str(cat_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trying textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_words = TextBlob(str(cat_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = da_words.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff =  [t[0] for t in tagged if t[1] == 'VB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'0': 1,\n",
       "             '1': 2,\n",
       "             '10': 1,\n",
       "             '11': 1,\n",
       "             '12': 1,\n",
       "             '13': 1,\n",
       "             '14': 1,\n",
       "             '15': 1,\n",
       "             '16': 1,\n",
       "             '17': 1,\n",
       "             '18': 1,\n",
       "             '19': 1,\n",
       "             '2': 1,\n",
       "             '20': 1,\n",
       "             '21': 1,\n",
       "             '22': 1,\n",
       "             '23': 1,\n",
       "             '24': 1,\n",
       "             '25': 1,\n",
       "             '26': 1,\n",
       "             '27': 1,\n",
       "             '28': 1,\n",
       "             '29': 1,\n",
       "             '3': 1,\n",
       "             '30': 1,\n",
       "             '4': 1,\n",
       "             '49764': 1,\n",
       "             '49765': 1,\n",
       "             '49766': 1,\n",
       "             '49767': 1,\n",
       "             '49768': 1,\n",
       "             '49769': 1,\n",
       "             '49770': 1,\n",
       "             '49771': 1,\n",
       "             '49772': 1,\n",
       "             '49773': 1,\n",
       "             '49774': 1,\n",
       "             '49775': 1,\n",
       "             '49776': 1,\n",
       "             '49777': 1,\n",
       "             '49778': 1,\n",
       "             '49779': 1,\n",
       "             '49780': 1,\n",
       "             '49781': 1,\n",
       "             '49782': 1,\n",
       "             '49783': 1,\n",
       "             '49784': 1,\n",
       "             '49785': 1,\n",
       "             '49786': 1,\n",
       "             '49787': 1,\n",
       "             '49788': 1,\n",
       "             '49789': 1,\n",
       "             '49790': 1,\n",
       "             '49791': 1,\n",
       "             '49792': 1,\n",
       "             '49793': 1,\n",
       "             '49794': 1,\n",
       "             '5': 1,\n",
       "             '6': 1,\n",
       "             '7': 1,\n",
       "             '8': 2,\n",
       "             '9': 1,\n",
       "             'a': 11,\n",
       "             'about': 1,\n",
       "             'activities': 1,\n",
       "             'after': 2,\n",
       "             'ah': 1,\n",
       "             'alright': 2,\n",
       "             'although': 1,\n",
       "             'always': 1,\n",
       "             'am': 1,\n",
       "             'an': 1,\n",
       "             'and': 7,\n",
       "             'annoying': 1,\n",
       "             'any': 1,\n",
       "             'are': 5,\n",
       "             'art': 1,\n",
       "             'artsy': 1,\n",
       "             'asleep': 1,\n",
       "             'at': 5,\n",
       "             'aw': 2,\n",
       "             'awesome': 2,\n",
       "             'b': 1,\n",
       "             'back': 1,\n",
       "             'bad': 1,\n",
       "             'badass': 1,\n",
       "             'bag': 1,\n",
       "             'be': 3,\n",
       "             'because': 1,\n",
       "             'been': 1,\n",
       "             'beer': 1,\n",
       "             'better': 1,\n",
       "             'bi': 1,\n",
       "             'boo': 1,\n",
       "             'bored': 1,\n",
       "             'both': 1,\n",
       "             'bound': 1,\n",
       "             'broken': 1,\n",
       "             'brown': 1,\n",
       "             'busy': 1,\n",
       "             'but': 2,\n",
       "             'can': 2,\n",
       "             'civil': 1,\n",
       "             'class': 2,\n",
       "             'cli': 1,\n",
       "             'climbing': 1,\n",
       "             'con': 1,\n",
       "             'cool': 1,\n",
       "             'coul': 1,\n",
       "             'couple': 1,\n",
       "             'cut': 1,\n",
       "             'd': 1,\n",
       "             'dad': 1,\n",
       "             'das': 1,\n",
       "             'day': 3,\n",
       "             'did': 3,\n",
       "             'different': 1,\n",
       "             'dinner': 1,\n",
       "             'do': 5,\n",
       "             'doin': 1,\n",
       "             'doing': 2,\n",
       "             'done': 1,\n",
       "             'dream': 1,\n",
       "             'dtype': 1,\n",
       "             'early': 2,\n",
       "             'either': 1,\n",
       "             'enough': 1,\n",
       "             'eve': 1,\n",
       "             'exams': 1,\n",
       "             'exciting': 1,\n",
       "             'exhausted': 1,\n",
       "             'f': 1,\n",
       "             'fall': 1,\n",
       "             'famil': 1,\n",
       "             'family': 1,\n",
       "             'finally': 2,\n",
       "             'first': 1,\n",
       "             'for': 1,\n",
       "             'forgive': 1,\n",
       "             'friends': 1,\n",
       "             'from': 1,\n",
       "             'fun': 2,\n",
       "             'get': 2,\n",
       "             'glad': 1,\n",
       "             'go': 2,\n",
       "             'going': 4,\n",
       "             'gon': 2,\n",
       "             'good': 2,\n",
       "             'goooood': 1,\n",
       "             'got': 3,\n",
       "             'gotten': 1,\n",
       "             'grad': 1,\n",
       "             'ha': 1,\n",
       "             'habit': 1,\n",
       "             'had': 3,\n",
       "             'haha': 3,\n",
       "             'hang': 1,\n",
       "             'have': 4,\n",
       "             'having': 1,\n",
       "             'hear': 1,\n",
       "             'hi': 4,\n",
       "             'high': 2,\n",
       "             'hiliiii': 1,\n",
       "             'home': 1,\n",
       "             'honeys': 1,\n",
       "             'hope': 2,\n",
       "             'hoping': 1,\n",
       "             'how': 4,\n",
       "             'i': 25,\n",
       "             'if': 1,\n",
       "             'impact': 1,\n",
       "             'in': 2,\n",
       "             'insi': 1,\n",
       "             'intern': 1,\n",
       "             'is': 1,\n",
       "             'isn': 1,\n",
       "             'it': 6,\n",
       "             'just': 4,\n",
       "             'kinda': 1,\n",
       "             'l': 1,\n",
       "             'late': 1,\n",
       "             'least': 1,\n",
       "             'legit': 1,\n",
       "             'length': 1,\n",
       "             'life': 1,\n",
       "             'like': 3,\n",
       "             'live': 1,\n",
       "             'll': 1,\n",
       "             'longest': 1,\n",
       "             'louis': 1,\n",
       "             'love': 1,\n",
       "             'm': 7,\n",
       "             'ma': 1,\n",
       "             'may': 1,\n",
       "             'me': 2,\n",
       "             'meet': 1,\n",
       "             'meow': 1,\n",
       "             'method': 1,\n",
       "             'might': 2,\n",
       "             'mile': 1,\n",
       "             'minutes': 1,\n",
       "             'miss': 1,\n",
       "             'mit': 1,\n",
       "             'mo': 1,\n",
       "             'mom': 1,\n",
       "             'more': 2,\n",
       "             'motorcycle': 1,\n",
       "             'mountains': 1,\n",
       "             'move': 1,\n",
       "             'much': 3,\n",
       "             'mucyh': 1,\n",
       "             'my': 6,\n",
       "             'na': 2,\n",
       "             'nah': 1,\n",
       "             'name': 1,\n",
       "             'nap': 1,\n",
       "             'nice': 1,\n",
       "             'night': 2,\n",
       "             'not': 1,\n",
       "             'notice': 1,\n",
       "             'now': 2,\n",
       "             'object': 1,\n",
       "             'of': 4,\n",
       "             'off': 1,\n",
       "             'oh': 3,\n",
       "             'on': 4,\n",
       "             'one': 2,\n",
       "             'only': 3,\n",
       "             'oo': 1,\n",
       "             'ooh': 1,\n",
       "             'ooohhhh': 1,\n",
       "             'other': 2,\n",
       "             'our': 1,\n",
       "             'out': 2,\n",
       "             'papers': 1,\n",
       "             'perfecto': 1,\n",
       "             'philistine': 1,\n",
       "             'plan': 1,\n",
       "             'plans': 1,\n",
       "             'pops': 1,\n",
       "             'pretty': 2,\n",
       "             'prof': 1,\n",
       "             're': 3,\n",
       "             'really': 3,\n",
       "             'recovered': 2,\n",
       "             'reeaally': 1,\n",
       "             'ridic': 1,\n",
       "             'righ': 1,\n",
       "             'right': 1,\n",
       "             'risk': 1,\n",
       "             's': 8,\n",
       "             'santa': 1,\n",
       "             'santacon': 1,\n",
       "             'say': 2,\n",
       "             'series.to_string': 1,\n",
       "             'sh': 1,\n",
       "             'she': 1,\n",
       "             'short': 1,\n",
       "             'simple': 1,\n",
       "             'ski': 1,\n",
       "             'skipping': 1,\n",
       "             'sleepverrry': 1,\n",
       "             'sn': 1,\n",
       "             'so': 4,\n",
       "             'somewhere': 1,\n",
       "             'soon': 1,\n",
       "             'sou': 1,\n",
       "             'soul': 1,\n",
       "             'sounds': 5,\n",
       "             'st': 1,\n",
       "             'stanford': 1,\n",
       "             'stl': 1,\n",
       "             'study': 1,\n",
       "             'stuff': 1,\n",
       "             'such': 1,\n",
       "             'sure': 2,\n",
       "             'sweet': 1,\n",
       "             't': 2,\n",
       "             'take': 1,\n",
       "             'test': 1,\n",
       "             'text': 1,\n",
       "             'th': 2,\n",
       "             'that': 8,\n",
       "             'the': 9,\n",
       "             'there': 4,\n",
       "             'these': 1,\n",
       "             'things': 1,\n",
       "             'think': 1,\n",
       "             'this': 2,\n",
       "             'those': 1,\n",
       "             'thursday': 1,\n",
       "             'time': 1,\n",
       "             'timing': 1,\n",
       "             'to': 14,\n",
       "             'today': 1,\n",
       "             'too': 3,\n",
       "             'tuesday': 1,\n",
       "             'u': 1,\n",
       "             'ultrasound': 1,\n",
       "             'undergrad': 1,\n",
       "             'used': 1,\n",
       "             've': 1,\n",
       "             'very': 2,\n",
       "             'w': 1,\n",
       "             'wa': 1,\n",
       "             'was': 5,\n",
       "             'wasn': 1,\n",
       "             'watch': 1,\n",
       "             'we': 3,\n",
       "             'went': 2,\n",
       "             'were': 3,\n",
       "             'what': 1,\n",
       "             'when': 1,\n",
       "             'where': 1,\n",
       "             'why': 1,\n",
       "             'will': 1,\n",
       "             'wish': 1,\n",
       "             'with': 3,\n",
       "             'would': 3,\n",
       "             'wow': 1,\n",
       "             'wowzas': 1,\n",
       "             'yeah': 3,\n",
       "             'yep': 1,\n",
       "             'you': 11,\n",
       "             'your': 1,\n",
       "             'youuu': 1,\n",
       "             'youuuu': 1,\n",
       "             '‚Äô': 11,\n",
       "             '‚ù§Ô∏è': 1,\n",
       "             '‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è': 1,\n",
       "             'Ôøºp.s': 1,\n",
       "             'üëçüèª': 1})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_words.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
